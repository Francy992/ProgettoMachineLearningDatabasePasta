{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variabilil globali\n",
    "width = 256\n",
    "height = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Far partire Visdom\n",
    "- Far funzionare accuracy.\n",
    "- Sistemare cartelle e codice.\n",
    "- Data agumentation\n",
    "- Finetuning -->(Ipotesi utilizzare alexnet implementata senza classificazione)\n",
    "- Aumentare il knn con data agumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AlexNet import *\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234);\n",
    "\n",
    "class ScenesDataset(Dataset):\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path\n",
    "        #carichiamo la lista dei file\n",
    "        #sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #print(\"self.images ha i seguenti elementi:\", len(self.images))\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        #print(\"Get item numero -->\", index)\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        #carichiamo l'immagine utilizzando PIL e facciamo il resize a 3 canali.\n",
    "        im = Image.open(path.join(self.base_path, f)).convert(\"RGB\")\n",
    "        \n",
    "        #Resize:\n",
    "        im = im.resize((width,height))\n",
    "        #se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "        \n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        #print(\"Mentre creo il tutto, label vale-->\", label, \", name vale -->\", f)\n",
    "        return {'image' : im, 'label':label, 'name': f}\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        #print(\"Ho invocato len, vale-->\", len(self.images))\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = ScenesDataset('Dataset','train.txt',transform=transforms.ToTensor())\n",
    "#for i in range(0, len(dataset)):\n",
    "    #print(dataset[i]['image'].shape, dataset[i]['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset\\\\Mealsolution_Barilla_Bio_Farinabiotipo21000g_Left.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a9b045a91ebe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transforms.ToTensor())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mm\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#accumuliamo la somma dei pixel canale per canale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#dividiamo per il numero di immagini moltiplicato per il numero di pixel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-0b47dde0d430>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#carichiamo l'immagine utilizzando PIL e facciamo il resize a 3 canali.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#Resize:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2548\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2549\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset\\\\Mealsolution_Barilla_Bio_Farinabiotipo21000g_Left.png'"
     ]
    }
   ],
   "source": [
    "#Normalizziamo i dati:\n",
    "#dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transforms.ToTensor())\n",
    "m = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    m+= np.array(sample['image'].sum(1).sum(1)) #accumuliamo la somma dei pixel canale per canale\n",
    "#dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "m=m/(len(dataset)*width*height)\n",
    "#procedura simile per calcolare la deviazione standard\n",
    "s = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    s+= np.array(((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1))\n",
    "s=np.sqrt(s/(len(dataset)*width*height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Medie\",m)\n",
    "print(\"Dev.Std.\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per evitare di  ricalcolare nuovamente le medie provare a salvarle qui.\n",
    "#Medie [0.53426401 0.5071057  0.50278591]\n",
    "#Dev.Std. [0.35348395 0.32069717 0.29183253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = transforms.Compose([transforms.ToTensor(), transforms.Normalize(m,s)])\n",
    "\n",
    "barilla_train = ScenesDataset('Dataset','train.txt',transform=transformss)\n",
    "barilla_test = ScenesDataset('Dataset','test.txt',transform=transformss)\n",
    "print()\n",
    "barilla_train_loader = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "net = models.alexnet(pretrained=True)\n",
    "classifier = nn.Sequential(*list(net.classifier.children())[:-1])\n",
    "net.classifier = classifier\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "def train_classification(model, train_loader, test_loader, lr=0.01, epochs=20, momentum=0.9):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    \n",
    "    \n",
    "    exp_name='experiment'\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    loss_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Loss', 'legend':['train','test']})\n",
    "    acc_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Accuracy','legend':['train','test']})\n",
    "    visdom_saver = VisdomSaver(envs=[exp_name])\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    for e in range(epochs):\n",
    "        #print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            #print(\"Secondo ciclo for.\")\n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            #print(\"Mode-->\",mode)\n",
    "            #print(\"Enumerate-->\", loaders[mode])\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "                y=Variable(batch['label'])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                    print(\"Con cuda\")\n",
    "                #else:\n",
    "                    #print(\"Senza cuda\")\n",
    "                output = model(x)\n",
    "                #print(type(output))\n",
    "                #print(output)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                #print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "                \n",
    "                if mode=='train':\n",
    "                    loss_logger.log(e+(i+1)/len(loader[mode]), loss_meter.value()[0], name=mode)\n",
    "                    acc_logger.log(e+(i+1)/len(loader[mode]), acc_meter.value()[0], name=mode)\n",
    "\n",
    "            loss_logger.log(e+1, loss_meter.value()[0], name=mode)\n",
    "            acc_logger.log(e+1, acc_meter.value()[0], name=mode)\n",
    "            \n",
    "            #print(\"Fine secondo ciclo for\")\n",
    "        print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))\n",
    "\n",
    "    print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con nostro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=1, train_loader = barilla_train_loader,\n",
    "                                                     test_loader = barilla_test_loader)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_logs_classification(logs):\n",
    "    training_losses, training_accuracies, test_losses, test_accuracies = \\\n",
    "    logs[0]['train'], logs[1]['train'], logs[0]['test'], logs[1]['test']\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(training_losses)\n",
    "    plt.plot(test_losses)\n",
    "    plt.legend(['Training Loss','Test Losses'])\n",
    "    plt.grid()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(training_accuracies)\n",
    "    plt.plot(test_accuracies)\n",
    "    plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_logs_classification(lenet_mnist_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classifier, samples):\n",
    "    right_pred = 0\n",
    "    for i in range(len(samples)):\n",
    "        pred_label = classifier.predict(samples[i][\"feature\"].cpu().detach().numpy().reshape(1, -1))\n",
    "        if pred_label[0] == samples[i][\"class\"]:\n",
    "            right_pred += 1\n",
    "            \n",
    "    return float(right_pred)/len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''def test_model_classification(model, test_loader = barilla_test_loader):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch in test_loader:\n",
    "        x=Variable(batch[\"image\"])\n",
    "        #applichiamo la funzione softmax per avere delle probabilità\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        pred = softmax(model(x)).data.cpu().numpy().copy()\n",
    "        gt = batch[\"label\"].cpu().numpy().copy()\n",
    "        #print(\"Pred-->\", pred, \", gt-->\", gt)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        #print(len(preds), len(gts))\n",
    "    return np.concatenate(preds),np.concatenate(gts)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net)\n",
    "print (\"Accuracy LeNet su DIGITS: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creazione feature dataset di training e test set\n",
    "def GetInputForDataframe(dataset, net):\n",
    "    #Presa ogni riga del dataloader li passa alla net senza attivare il layer di classificazione\n",
    "    feature_dataset = []\n",
    "    for i, dataset_train in enumerate(dataset):\n",
    "        #print(i, \"-->\",dataset_train['image'].shape, dataset_train['label'])\n",
    "        #print(dataset_train['label'])\n",
    "        x=Variable(dataset_train['image'], requires_grad=False)\n",
    "        y=Variable(dataset_train['label'])\n",
    "        x, y = x.cpu(), y.cpu()\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            #print(\"Con cuda\")\n",
    "        output = net(x)\n",
    "        feature_dataset.append({\"label\": dataset_train['label'], \"feature\":output, \"name\": dataset_train['name']})\n",
    "\n",
    "    #Trasformiamo il tensor in una matrice\n",
    "    print(len(feature_dataset), len(feature_dataset[0]))\n",
    "    feature_dataset_matrix = np.zeros((len(feature_dataset), len(feature_dataset[0][\"feature\"][0])))\n",
    "    \n",
    "    #Qui abbiamo nelle righe tutte le immagini, nella lable feature tutte le 9000 colonne, ossia le feature.\n",
    "    print(len(feature_dataset), len(feature_dataset[0][\"feature\"][0]))\n",
    "    print(feature_dataset[0][\"label\"][0])\n",
    "    print(feature_dataset[1][\"label\"][0])\n",
    "\n",
    "    label_array = np.zeros(len(feature_dataset))\n",
    "    for i in range(0, len(feature_dataset)):#302\n",
    "        for j in range(0, len(feature_dataset[0][\"feature\"][0])):#9206\n",
    "            if j == 0:#salviamo la y finale nella colonna 0 della riga x.\n",
    "                #feature_dataset_matrix[j][i] = feature_dataset[j]['label'][0]\n",
    "                label_array[i] = feature_dataset[i]['label'][0]\n",
    "                print(i, end= \" \")\n",
    "            #else:\n",
    "            #print( i, j, end=\" \")\n",
    "            #Qua le estsrapoliamo DALLa LABEL feature e le sistemiamo in una matrice da dare in input al knn.\n",
    "            feature_dataset_matrix[i][j] =feature_dataset[i][\"feature\"][0][j] \n",
    "\n",
    "    print(len(feature_dataset_matrix), len(feature_dataset_matrix[0]), len(label_array))\n",
    "    print(feature_dataset[0][\"feature\"][0])\n",
    "    return feature_dataset_matrix, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "knn_1 = KNN(n_neighbors=1)\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "input_for_datafram_train, label_array_train = GetInputForDataframe(barilla_train_loader, net)\n",
    "\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "#print(label_array[0], df.drop(0, axis = 1))\n",
    "knn_1.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prendiamo le feature di una riga di test:\n",
    "feature_test = []\n",
    "for i, dataset_test in enumerate(barilla_test_loader):\n",
    "    if i == 129:\n",
    "        x=Variable(dataset_test['image'], requires_grad=False)\n",
    "        y=Variable(dataset_test['label'])\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            #print(\"Con cuda\")\n",
    "        feature_test_0 = {\"feature\": net(x), \"class\": dataset_test['label']}\n",
    "    #print(dataset_test['label'])\n",
    "    x=Variable(dataset_test['image'], requires_grad=False)\n",
    "    y=Variable(dataset_test['label'])\n",
    "    if torch.cuda.is_available():\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        #print(\"Con cuda\")\n",
    "        #Questo è un vettore, facciamo di 0 perchè le altre voci non ci interessano al momento.\n",
    "    feature_test.append({\"feature\": net(x), \"class\": dataset_test['label']})\n",
    "#Proviamo la predizione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn_1.fit(df, label_array_train)\n",
    "index = 15\n",
    "print(len(feature_test[index][\"feature\"][0]), feature_test[22][\"feature\"])\n",
    "z = knn_1.predict(feature_test[index][\"feature\"].cpu().detach().numpy().reshape(1, -1))\n",
    "print(len(df.drop(0, axis = 1)), len(df[0]))\n",
    "print(z[0] , \" - real -->\", feature_test[index][\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(knn_1, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB: Guardare per bene i nomi, probabilmente c'è qualche errore e decommentare nei vari posti il torch.cuda.is_available.\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "knn_1 = KNN(n_neighbors=1)\n",
    "input_for_datafram_test, label_array_input = GetInputForDataframe(barilla_test_loader, net)\n",
    "df = pd.DataFrame(input_for_datafram_test)\n",
    "#print(label_array[0], df.drop(0, axis = 1))\n",
    "knn_1.fit(df, label_array)\n",
    "#Prendiamo le feature di una riga di test:\n",
    "feature_test = []\n",
    "for i, dataset_test in enumerate(barilla_test_loader):\n",
    "    #if i == 129:\n",
    "    #print(dataset_test['label'])\n",
    "    x=Variable(dataset_test['image'], requires_grad=False)\n",
    "    y=Variable(dataset_test['label'])\n",
    "    #if torch.cuda.is_available():\n",
    "        #x, y = x.cuda(), y.cuda()\n",
    "        #print(\"Con cuda\")\n",
    "    feature_test_0 = net(x, False)\n",
    "    feature_test.append(net(x, False)[0])\n",
    "#Proviamo la predizione\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy sul training set: %0.2f\" % knn_1.score(df, label_array))\n",
    "print(\"Accuracy sul test set: %0.2f\" % knn_1.score(data_test.drop('C',axis=1), data_test.C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "#Questa è la funzione per i dataset classici.\n",
    "def train_classification_special(model, lr=0.01, epochs=20, momentum=0.9, \\\n",
    "    train_loader=mnist_train_loader, test_loader=mnist_test_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    for e in range(epochs):\n",
    "        print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            print(\"Secondo ciclo for.\")\n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            print(\"Mode-->\",mode)\n",
    "            print(\"Enumerate-->\", loaders[mode])\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                x=Variable(batch[0], requires_grad=(mode=='train'))\n",
    "                #print(\"x shape-->\",x.shape)\n",
    "                y=Variable(batch[1])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                output = model(x)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "            print(\"Fine secondo ciclo for\")\n",
    "        print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))\n",
    "\n",
    "    print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Created by: BoyuanJiang\n",
    "# College of Information Science & Electronic Engineering,ZheJiang University\n",
    "# Email: ginger188@gmail.com\n",
    "# Copyright (c) 2017\n",
    "\n",
    "# @Time    :17-8-27 21:25\n",
    "# @FILE    :matching_networks.py\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def convLayer(in_channels, out_channels, keep_prob=0.0):\n",
    "    \"\"\"3*3 convolution with padding,ever time call it the output size become half\"\"\"\n",
    "    cnn_seq = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout(keep_prob)\n",
    "    )\n",
    "    return cnn_seq\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, layer_size=64, num_channels=1, keep_prob=1.0, image_size=28):\n",
    "        super(Classifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Build a CNN to produce embeddings\n",
    "        :param layer_size:64(default)\n",
    "        :param num_channels:\n",
    "        :param keep_prob:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        self.layer1 = convLayer(num_channels, layer_size, keep_prob)\n",
    "        self.layer2 = convLayer(layer_size, layer_size, keep_prob)\n",
    "        self.layer3 = convLayer(layer_size, layer_size, keep_prob)\n",
    "        self.layer4 = convLayer(layer_size, layer_size, keep_prob)\n",
    "\n",
    "        finalSize = int(math.floor(image_size / (2 * 2 * 2 * 2)))\n",
    "        self.outSize = finalSize * finalSize * layer_size\n",
    "\n",
    "    def forward(self, image_input):\n",
    "        \"\"\"\n",
    "        Use CNN defined above\n",
    "        :param image_input:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.layer1(image_input)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionalClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionalClassify, self).__init__()\n",
    "\n",
    "    def forward(self, similarities, support_set_y):\n",
    "        \"\"\"\n",
    "        Products pdfs over the support set classes for the target set image.\n",
    "        :param similarities: A tensor with cosine similarites of size[batch_size,sequence_length]\n",
    "        :param support_set_y:[batch_size,sequence_length,classes_num]\n",
    "        :return: Softmax pdf shape[batch_size,classes_num]\n",
    "        \"\"\"\n",
    "        softmax = nn.Softmax()\n",
    "        softmax_similarities = softmax(similarities)\n",
    "        preds = softmax_similarities.unsqueeze(1).bmm(support_set_y).squeeze()\n",
    "        return preds\n",
    "\n",
    "\n",
    "class DistanceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This model calculates the cosine distance between each of the support set embeddings and the target image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "\n",
    "    def forward(self, support_set, input_image):\n",
    "        \"\"\"\n",
    "        forward implement\n",
    "        :param support_set:the embeddings of the support set images.shape[sequence_length,batch_size,64]\n",
    "        :param input_image: the embedding of the target image,shape[batch_size,64]\n",
    "        :return:shape[batch_size,sequence_length]\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        for support_image in support_set:\n",
    "            sum_support = torch.sum(torch.pow(support_image, 2), 1)\n",
    "            support_manitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n",
    "            cosine_similarity = dot_product * support_manitude\n",
    "            similarities.append(cosine_similarity)\n",
    "        similarities = torch.stack(similarities)\n",
    "        return similarities.t()\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, layer_size, batch_size, vector_dim,use_cuda):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        Initial a muti-layer Bidirectional LSTM\n",
    "        :param layer_size: a list of each layer'size\n",
    "        :param batch_size: \n",
    "        :param vector_dim: \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = layer_size[0]\n",
    "        self.vector_dim = vector_dim\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.use_cuda = use_cuda\n",
    "        self.lstm = nn.LSTM(input_size=self.vector_dim, num_layers=self.num_layer, hidden_size=self.hidden_size,\n",
    "                            bidirectional=True)\n",
    "        self.hidden = self.init_hidden(self.use_cuda)\n",
    "\n",
    "    def init_hidden(self,use_cuda):\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False).cuda(),\n",
    "                    Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False),\n",
    "                    Variable(torch.zeros(self.lstm.num_layers * 2, self.batch_size, self.lstm.hidden_size),requires_grad=False))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == Variable:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # self.hidden = self.init_hidden(self.use_cuda)\n",
    "        self.hidden = self.repackage_hidden(self.hidden)\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, keep_prob=0.0, batch_size=32, num_channels=1, learning_rate=1e-3, fce=False, num_classes_per_set=20, \\\n",
    "                 num_samples_per_class=1, image_size=28, use_cuda=True):\n",
    "        \"\"\"\n",
    "        This is our main network\n",
    "        :param keep_prob: dropout rate\n",
    "        :param batch_size:\n",
    "        :param num_channels:\n",
    "        :param learning_rate:\n",
    "        :param fce: Flag indicating whether to use full context embeddings(i.e. apply an LSTM on the CNN embeddings)\n",
    "        :param num_classes_per_set:\n",
    "        :param num_samples_per_class:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.keep_prob = keep_prob\n",
    "        self.num_channels = num_channels\n",
    "        self.learning_rate = learning_rate\n",
    "        self.fce = fce\n",
    "        self.num_classes_per_set = num_classes_per_set\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.image_size = image_size\n",
    "        self.g = Classifier(layer_size=64, num_channels=num_channels, keep_prob=keep_prob, image_size=image_size)\n",
    "        self.dn = DistanceNetwork()\n",
    "        self.classify = AttentionalClassify()\n",
    "        if self.fce:\n",
    "            self.lstm = BidirectionalLSTM(layer_size=[32], batch_size=self.batch_size, vector_dim=self.g.outSize,use_cuda=use_cuda)\n",
    "\n",
    "    def forward(self, support_set_images, support_set_y_one_hot, target_image, target_y):\n",
    "        \"\"\"\n",
    "        Main process of the network\n",
    "        :param support_set_images: shape[batch_size,sequence_length,num_channels,image_size,image_size]\n",
    "        :param support_set_y_one_hot: shape[batch_size,sequence_length,num_classes_per_set]\n",
    "        :param target_image: shape[batch_size,num_channels,image_size,image_size]\n",
    "        :param target_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # produce embeddings for support set images\n",
    "        encoded_images = []\n",
    "        for i in np.arange(support_set_images.size(1)):\n",
    "            gen_encode = self.g(support_set_images[:, i, :, :])\n",
    "            encoded_images.append(gen_encode)\n",
    "\n",
    "        # produce embeddings for target images\n",
    "        gen_encode = self.g(target_image)\n",
    "        encoded_images.append(gen_encode)\n",
    "        output = torch.stack(encoded_images)\n",
    "\n",
    "        # use fce?\n",
    "        if self.fce:\n",
    "            outputs = self.lstm(output)\n",
    "\n",
    "        # get similarities between support set embeddings and target\n",
    "        similarites = self.dn(support_set=output[:-1], input_image=output[-1])\n",
    "\n",
    "        # produce predictions for target probabilities\n",
    "        preds = self.classify(similarites, support_set_y=support_set_y_one_hot)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        values, indices = preds.max(1)\n",
    "        accuracy = torch.mean((indices.squeeze() == target_y).float())\n",
    "        crossentropy_loss = F.cross_entropy(preds, target_y.long())\n",
    "\n",
    "        return accuracy, crossentropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_network = MatchingNetwork()\n",
    "lenet_mnist_predictions, lenet_mnist_gt = train_classification(matching_network)\n",
    "print (\"Accuracy LeNet su DIGITS: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
