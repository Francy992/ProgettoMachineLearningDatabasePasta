{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variabilil globali\n",
    "width = 256\n",
    "height = 256\n",
    "mean_pre_trained =[0.485, 0.456, 0.406]\n",
    "std_pre_trained =[0.229, 0.224, 0.225]\n",
    "num_classes = 153"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Far partire Visdom\n",
    "- Data agumentation\n",
    "- Aumentare il knn con data agumentation.\n",
    "- Allenare con nostre foto.\n",
    "\n",
    "\n",
    "\n",
    "# Domande al prof:\n",
    "- Perchè cambiando le dimensioni, aumentandole e diminuendole, l'accuracy cambia anche di molto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "from torchnet.logger import VisdomPlotLogger, VisdomSaver\n",
    "from torchnet.meter import AverageValueMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe per la gestione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234);\n",
    "\n",
    "class ScenesDataset(Dataset):\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path\n",
    "        #carichiamo la lista dei file\n",
    "        #sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #print(\"self.images ha i seguenti elementi:\", len(self.images))\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        #print(\"Get item numero -->\", index)\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        #carichiamo l'immagine utilizzando PIL e facciamo il resize a 3 canali.\n",
    "        im = Image.open(path.join(self.base_path, f)).convert(\"RGB\")\n",
    "        \n",
    "        #Resize:\n",
    "        im = im.resize((width,height))\n",
    "        #se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)       \n",
    "        \n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        #print(\"Mentre creo il tutto, label vale-->\", label, \", name vale -->\", f)\n",
    "        return {'image' : im, 'label':label, 'name': f}\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        #print(\"Ho invocato len, vale-->\", len(self.images))\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione che ci restituisce media e varianza di un dataset.\n",
    "- Prende in input il dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_devst(dataset):\n",
    "    m = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        m+= np.array(sample['image'].sum(1).sum(1)) #accumuliamo la somma dei pixel canale per canale\n",
    "    #dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "    m=m/(len(dataset)*width*height)\n",
    "    #procedura simile per calcolare la deviazione standard\n",
    "    s = np.zeros(3)\n",
    "    for sample in dataset:\n",
    "        s+= np.array(((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1))\n",
    "    s=np.sqrt(s/(len(dataset)*width*height))\n",
    "    print(\"Medie\",m)\n",
    "    print(\"Dev.Std.\",s)\n",
    "    return m, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prendiamo media e varianza per normalizzare successivamente i dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ScenesDataset('Dataset_base','train.txt',transform=transforms.ToTensor())\n",
    "#m, s = get_mean_devst(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per calcolare l'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prende in input l'array di feature e il classificatore(knn.) \n",
    "def accuracy(classifier, samples):\n",
    "    right_pred = 0\n",
    "    for i in range(len(samples)):\n",
    "        pred_label = classifier.predict(samples[i][\"feature\"].cpu().detach().numpy().reshape(1, -1))\n",
    "        if pred_label[0] == samples[i][\"label\"]:\n",
    "            right_pred += 1\n",
    "            \n",
    "    return float(right_pred)/len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per estrazione feature:\n",
    "- In input dataset (dataloader) e rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset, net):\n",
    "    #Presa ogni riga del dataloader li passa alla net senza attivare il layer di classificazione\n",
    "    feature_dataset = []\n",
    "    print(\"Avviato extract_feature.\")\n",
    "    for i, dataset_train in enumerate(dataset):\n",
    "        x=Variable(dataset_train['image'], requires_grad=False)\n",
    "        y=Variable(dataset_train['label'])\n",
    "        x, y = x.cpu(), y.cpu()\n",
    "        #if torch.cuda.is_available():\n",
    "            #x, y = x.cuda(), y.cuda()\n",
    "            #print(\"Con cuda\")\n",
    "        output = net(x)\n",
    "        feature_dataset.append({\"label\": dataset_train['label'], \"feature\":output, \"name\": dataset_train['name']})\n",
    "    return feature_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzione per creare l'input per l'oggetto dataframe:\n",
    "- In input dataset (dataloader) e rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset, net):\n",
    "    print(\"Avviato get_dataframe.\")\n",
    "    feature_dataset = extract_features(dataset, net)  \n",
    "    feature_dataset_matrix = np.zeros((len(feature_dataset), len(feature_dataset[0][\"feature\"][0])))    \n",
    "    #Qui abbiamo nelle righe tutte le immagini, nella lable feature tutte le 9000 colonne, ossia le feature.\n",
    "    label_array = np.zeros(len(feature_dataset))\n",
    "    for i in range(0, len(feature_dataset)):#302\n",
    "        for j in range(0, len(feature_dataset[0][\"feature\"][0])):#9206\n",
    "            if j == 0:#salviamo la y finale nell'array label_array\n",
    "                label_array[i] = feature_dataset[i]['label'][0]\n",
    "                #print(i, end= \" \")\n",
    "            feature_dataset_matrix[i][j] =feature_dataset[i][\"feature\"][0][j] \n",
    "\n",
    "    return feature_dataset_matrix, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_classification(model, train_loader, test_loader, lr=0.001, epochs=20, momentum=0.8, exp_name = 'experiment' ):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(),lr, momentum=momentum)\n",
    "    loaders = {'train':train_loader, 'test':test_loader}\n",
    "    losses = {'train':[], 'test':[]}\n",
    "    accuracies = {'train':[], 'test':[]}\n",
    "    \n",
    "    \n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    loss_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Loss', 'legend':['train','test']})\n",
    "    acc_logger = VisdomPlotLogger('line', env=exp_name, opts={'title': 'Accuracy','legend':['train','test']})\n",
    "    visdom_saver = VisdomSaver(envs=[exp_name])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model=model.cuda()\n",
    "    for e in range(epochs):\n",
    "        #print(\"Primo ciclo for.\")\n",
    "        for mode in ['train', 'test']:\n",
    "            #print(\"Secondo ciclo for.\")\n",
    "            \n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            \n",
    "            if mode=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            samples = 0\n",
    "            #print(\"Mode-->\",mode)\n",
    "            #print(\"Enumerate-->\", loaders[mode])\n",
    "            for i, batch in enumerate(loaders[mode]):\n",
    "                #trasformiamo i tensori in variabili\n",
    "                x=Variable(batch['image'], requires_grad=(mode=='train'))\n",
    "                y=Variable(batch['label'])\n",
    "                if torch.cuda.is_available():\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "                    print(\"Con cuda\")\n",
    "                #else:\n",
    "                    #print(\"Senza cuda\")\n",
    "                output = model(x)\n",
    "                #print(type(output))\n",
    "                #print(output)\n",
    "                l = criterion(output,y)\n",
    "                if mode=='train':\n",
    "                    l.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                #print(\"L-->\",l.item())\n",
    "                acc = accuracy_score(y.cpu().data,output.cpu().max(1)[1].data)\n",
    "                epoch_loss+=l.data.item()*x.shape[0]\n",
    "                epoch_acc+=acc*x.shape[0]\n",
    "                samples+=x.shape[0]\n",
    "                print (\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "                (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss/samples, epoch_acc/samples),\n",
    "                epoch_loss/samples,\n",
    "                epoch_acc/samples,\n",
    "                losses[mode].append(epoch_loss))\n",
    "                accuracies[mode].append(epoch_acc)\n",
    "                n = batch['image'].shape[0]\n",
    "                loss_meter.add(l.item()*n,n)\n",
    "                acc_meter.add(acc*n,n)\n",
    "                loss_logger.log(e+(i+1)/len(loaders[mode]), loss_meter.value()[0], name=mode)\n",
    "                acc_logger.log(e+(i+1)/len(loaders[mode]), acc_meter.value()[0], name=mode)\n",
    "\n",
    "\n",
    "            loss_logger.log(e+1, loss_meter.value()[0], name=mode)\n",
    "            acc_logger.log(e+1, acc_meter.value()[0], name=mode)\n",
    "            \n",
    "            #print(\"Fine secondo ciclo for\")\n",
    "        print(\"\\r[%s] Epoch %d/%d. Iteration %d/%d. Loss: %0.2f. Accuracy: %0.2f\\t\\t\\t\\t\\t\" % \\\n",
    "        (mode, e+1, epochs, i, len(loaders[mode]), epoch_loss, epoch_acc))\n",
    "\n",
    "    print(\"Ho finito.\")\n",
    "    #restituiamo il modello e i vari log\n",
    "    return model, (losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test_model_classification(model, test_loader):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    for batch in test_loader:\n",
    "        x=Variable(batch[\"image\"])\n",
    "        x = x.cpu()\n",
    "        #applichiamo la funzione softmax per avere delle probabilità\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            model.cuda()\n",
    "        pred = softmax(model(x)).data.cpu().numpy().copy()\n",
    "        gt = batch[\"label\"].cpu().numpy().copy()\n",
    "        #print(\"Pred-->\", pred, \", gt-->\", gt)\n",
    "        preds.append(pred)\n",
    "        gts.append(gt)\n",
    "        #print(len(preds), len(gts))\n",
    "    return np.concatenate(preds),np.concatenate(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo Oggetto Dataset e dataloader, sia base che data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_pre_trained,std_pre_trained)])\n",
    "barilla_train = ScenesDataset('Dataset_base','train.txt',transform=transformss)\n",
    "barilla_test = ScenesDataset('Dataset_base','test.txt',transform=transformss)\n",
    "barilla_train_loader = torch.utils.data.DataLoader(barilla_train, batch_size=10, num_workers=0, shuffle=True)\n",
    "barilla_test_loader = torch.utils.data.DataLoader(barilla_test, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_pre_trained,std_pre_trained)])\n",
    "barilla_train_da = ScenesDataset('Dataset_data_augmentation','train_data_augmentation.txt',transform=transformss)\n",
    "barilla_test_da = ScenesDataset('Dataset_data_augmentation','test_data_augmentation.txt',transform=transformss)\n",
    "barilla_train_loader_da = torch.utils.data.DataLoader(barilla_train_da, batch_size=10, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_da = torch.utils.data.DataLoader(barilla_test_da, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet con i parametri pre-allenati e un numero di classi finali pari a 153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "net.classifier[6] = nn.Linear(4096, num_classes) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test = extract_features(barilla_test_loader_OB, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet con i parametri pre-allenati e un numero di classi finali pari a 1000 (valore di default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test = extract_features(barilla_test_loader_OB, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet riallenando solamente l'ultimo layer con un numero di classi pari a 153 e 20 epoche di allenamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "net.classifier[6] = nn.Linear(4096, num_classes) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"last_layer_training_153classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader,\n",
    "                                                     test_loader = barilla_test_loader, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test = extract_features(barilla_test_loader_OB, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet riallenando gli ultimi due layer con un numero di classi pari a 153 e 20 epoche di allenamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "net.classifier[4] = nn.Linear(4096, 4096) #Numero esatto di classi nel nostro dataset.\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"last_two_layers_training_153classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader,\n",
    "                                                     test_loader = barilla_test_loader, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test = extract_features(barilla_test_loader_OB, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rialleniamo alexnet con 153 classi in output. 20 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet()\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"retraining_all_layer_153classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader,\n",
    "                                                     test_loader = barilla_test_loader, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB = torch.utils.data.DataLoader(barilla_train, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB = torch.utils.data.DataLoader(barilla_test, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test = extract_features(barilla_test_loader_OB, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br><br><br> <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo Oggetto Dataset e dataloader, sia base che data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformss = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean_pre_trained,std_pre_trained)])\n",
    "barilla_train_da = ScenesDataset('Dataset_data_augmentation','train_data_augmentation.txt',transform=transformss)\n",
    "barilla_test_da = ScenesDataset('Dataset_data_augmentation','test_data_augmentation.txt',transform=transformss)\n",
    "barilla_train_loader_da = torch.utils.data.DataLoader(barilla_train_da, batch_size=10, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_da = torch.utils.data.DataLoader(barilla_test_da, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet con i parametri pre-allenati e un numero di classi finali pari a 153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57630681"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy aLexNet di train su barillatestloader: 0.01\n"
     ]
    }
   ],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader_da)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy aLexNet di test su barillatestloader: 0.01\n"
     ]
    }
   ],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader_da)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avviato get_dataframe.\n",
      "Avviato extract_feature.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 2.89 GiB already allocated; 486.40 KiB free; 15.84 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ea8dd0e0ba0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0minput_for_datafram_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_array_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbarilla_train_loader_OB_da\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_for_datafram_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mknn_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_array_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-fbef828a767e>\u001b[0m in \u001b[0;36mget_dataframe\u001b[1;34m(dataset, net)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Avviato get_dataframe.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeature_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mfeature_dataset_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Qui abbiamo nelle righe tutte le immagini, nella lable feature tutte le 9000 colonne, ossia le feature.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-15fcc162030d>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(dataset, net)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;31m#print(\"Con cuda\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 2.89 GiB already allocated; 486.40 KiB free; 15.84 MiB cached)"
     ]
    }
   ],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    net = net.cuda()\n",
    "#    torch.cuda.empty_cache()\n",
    "net.cpu()\n",
    "net.eval()\n",
    "\n",
    "barilla_train_loader_OB_da = torch.utils.data.DataLoader(barilla_train_da, batch_size=1, num_workers=0, shuffle=True)\n",
    "barilla_test_loader_OB_da = torch.utils.data.DataLoader(barilla_test_da, batch_size=1, num_workers=0)\n",
    "\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB_da, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test_da = extract_features(barilla_test_loader_OB_da, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test_da))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test_da))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet con i parametri pre-allenati e un numero di classi finali pari a 1000 (valore di default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader_da)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader_da)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    net = net.cuda()\n",
    "#    torch.cuda.empty_cache()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB_da, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test_da = extract_features(barilla_test_loader_OB_da, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test_da))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test_da))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet riallenando solamente l'ultimo layer con un numero di classi pari a 153 e 20 epoche di allenamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"last_layer_training_153classes_fulldataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader_da,\n",
    "                                                     test_loader = barilla_test_loader_da, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader_da)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader_da)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    net = net.cuda()\n",
    "#    torch.cuda.empty_cache()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB_da, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test_da = extract_features(barilla_test_loader_OB_da, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test_da))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test_da))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizziamo AlexNet riallenando gli ultimi due layer con un numero di classi pari a 153 e 20 epoche di allenamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "net.classifier[4] = nn.Linear(4096, 4096) #Numero esatto di classi nel nostro dataset.\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"last_two_layer_training_153classes_fulldataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader_da,\n",
    "                                                     test_loader = barilla_test_loader_da, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader_da)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader_da)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    net = net.cuda()\n",
    "#    torch.cuda.empty_cache()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB_da, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test_da = extract_features(barilla_test_loader_OB_da, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test_da))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test_da))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rialleniamo alexnet con 153 classi in output. 20 epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet()\n",
    "net.classifier[6] = nn.Linear(4096, 153) #Numero esatto di classi nel nostro dataset.\n",
    "sum([p.numel() for p in net.parameters()])\n",
    "name_save = \"retraining_alexnet_153classes_fulldataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist, lenet_mnist_logs = train_classification(net, epochs=20, train_loader = barilla_train_loader_da,\n",
    "                                                     test_loader = barilla_test_loader_da, exp_name = name_save)\n",
    "torch.save(net.state_dict(), \"./\" + name_save + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy senza usare il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_train_loader_da)\n",
    "print (\"Accuracy aLexNet di train su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_mnist_predictions, lenet_mnist_gt = test_model_classification(net, barilla_test_loader_da)\n",
    "print (\"Accuracy aLexNet di test su barillatestloader: %0.2f\" % \\\n",
    "accuracy_score(lenet_mnist_gt,lenet_mnist_predictions.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo il dataframe e il knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_3 = KNN(n_neighbors=3)\n",
    "knn_5 = KNN(n_neighbors=5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "net.eval()\n",
    "\n",
    "input_for_datafram_train, label_array_train = get_dataframe(barilla_train_loader_OB_da, net)\n",
    "df = pd.DataFrame(input_for_datafram_train)\n",
    "knn_1.fit(df, label_array_train)\n",
    "knn_3.fit(df, label_array_train)\n",
    "knn_5.fit(df, label_array_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_test_da = extract_features(barilla_test_loader_OB_da, net)\n",
    "print(\"Accuracy con rete preallenata e dataset base.\")\n",
    "print(\"1nn-->\",accuracy(knn_1, feature_test_da))\n",
    "print(\"3nn-->\",accuracy(knn_3, feature_test_da))\n",
    "print(\"5nn-->\",accuracy(knn_5, feature_test_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
